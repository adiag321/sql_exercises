Every night between 7 pm and midnight, two computing jobs from two different sources are randomly started with each one lasting an hour.

Unfortunately, when the jobs simultaneously run, they cause a failure in some of the companyâ€™s other nightly jobs, resulting in downtime for the company that costs $1000. 

The CEO, who has enough time today to hear one word, needs a single number representing the annual (365 days) cost of this problem.

Note: Write a function to simulate this problem and output an estimated cost 

Bonus - How would you solve this using probability?
--------------------------------------------------------------------------------------------------------------------------------------
import numpy as np

def simulate_overlap():
    task1 = np.random.randint(0, 300, size = 10000)
    task2 = np.random.randint(0, 300, size = 10000)
    overlap=0
    for i in range(1,len(task1)):
    
        fp=task1[i]
        sp=task2[i]
    
        if (fp<sp and sp<=fp+60):
            overlap=overlap+1
        elif (sp<fp and fp<=sp+60):
            overlap=overlap+1
        elif (sp==fp):
            overlap=overlap+1
    return((overlap/len(task1))*365*1000)
