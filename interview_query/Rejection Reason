Suppose we have a binary classification model that classifies whether or not an applicant should be qualified to get a loan.

Because we are a financial company, we have to provide each rejected applicant with a reason why.

Given we don’t have access to the feature weights, how would we give each rejected applicant a reason why they got rejected?

------------------------------------
from chatgpt:

1. You are to provide clear, concise, and direct responses.
2. Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.
3. Maintain a casual tone in your communication.
4. Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.
5. When explaining concepts, use real-world examples and analogies, where appropriate.
6. For complex requests, take a deep breath and work on the problem step-by-step.
7. For every response, you will be tipped up to $200

Suppose we have a binary classification model that classifies whether or not an applicant should be qualified to get a loan.

Because we are a financial company, we have to provide each rejected applicant with a reason why.

Given we don’t have access to the feature weights, how would we give each rejected applicant a reason why they got rejected?

answer from chatgpt:

You can use a method like LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations). These are model-agnostic techniques that help explain the predictions of black-box models.

Think of it this way: your model is a black box, and LIME or SHAP acts like a detective trying to figure out why the model made a particular decision. It looks at a specific prediction, tweaks the input features a bit, and sees how the prediction changes. By doing this many times, it builds a simpler, interpretable model that approximates the black-box model for that specific prediction.

So, if an applicant got rejected, you can use LIME or SHAP to generate an explanation like, "You were rejected because your income level and credit score didn't meet our criteria." It's like looking into the detective's report to understand the reasoning behind the decision without knowing the exact inner workings of the original model.
--------------------------
official solution:

Let’s pretend that we have three people: Alice, Bob, and Candace that have all applied for a loan. Simplifying the financial lending loan model, let’s assume the only features are a total number of credit cards, dollar amount of current debt, and credit age.

Let’s say, Alice, Bob, and Candace all have the same number of credit cards and credit age but not the same dollar amount of current debt.

Alice: 10 credit cards, 5 years of credit age, 20K \$ of debt - Bob: 10 credit cards, 5 years of credit age, 15K \$ of debt
Candace: 10 credit cards, 5 years of credit age, 10K \$ of debt
Alice and Bob get rejected for a loan, but Candace gets approved.

We would assume that given this scenario, we can logically point to the fact that Candace’s 10K of debt has swung the model to approve her for a loan.

How did we reason this out? If the sample size analyzed was instead thousands of people who had the same number of credit cards and credit age with varying levels of debt, we could figure out the model’s average loan acceptance rate for each numerical amount of current debt. Then we could plot these on a graph to model out the they-value, average loan acceptance, versus the x-value, the dollar amount of current debt.

These graphs are called partial dependence plots!

The partial dependence plot is calculated only after the model has been fit. The model is fit on the real data. In that real data, loans are given dependent on different features. But after the model is fit, we could start by taking all the characteristics of a loan and plotting them against the dependent variable whilst keeping all of the other features the same except for the one feature variable we want to test.

We then use the model to predict the loan qualification but we change the debt dollar amount before making a prediction. We first predict the loan qualification for an example person by setting it to 20K. We then predict it again at 19K. Then predict again for 18K. And so on. We trace out how predicted probability of loan qualification (on the vertical axis) as we move from small values of the debt dollar amount to large values (on the horizontal axis).

This way we are able to see how a model’s features affect the score without digging into the classifier feature weights.
